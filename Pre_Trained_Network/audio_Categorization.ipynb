{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "IMPORT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\basti\\Anaconda3\\envs\\ProjetE4\\lib\\site-packages\\pydub\\utils.py:170: RuntimeWarning: Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\n",
      "  warn(\"Couldn't find ffmpeg or avconv - defaulting to ffmpeg, but may not work\", RuntimeWarning)\n"
     ]
    }
   ],
   "source": [
    "#Datas & plots\n",
    "import splitfolders\n",
    "import wave\n",
    "from pydub import AudioSegment\n",
    "from pydub.playback import play\n",
    "import random\n",
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from scipy.io import wavfile\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Les données\n",
    "\n",
    "- Utilisation du package splitfolders pour séparer les données en : train_set, validation_set et test_set.\n",
    "- Récupération des caractéristiques de nos données audios.\n",
    "- Création d'un csv mis a disposition sur le Drive avec toutes les caractéristiques de chaque fichier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Copying files: 471 files [00:03, 154.68 files/s]\n"
     ]
    }
   ],
   "source": [
    "#Split des données aux labels connus pour créer un train_set,un validation_set et un test_set\n",
    "splitfolders.ratio('./Data/Known_datas',output=\"./Data/Train\",seed=1337,ratio=(.6,.2,.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fréquence d'échantillonnage = 4000\n",
      "Type de son = Mono\n",
      "BitRate = 16\n"
     ]
    }
   ],
   "source": [
    "#Récupération de la fréquence d'échantillonnage (framerate), nombre de channel ou type de son (stéréo/mono), et le bitrate en octet (1 = 8 bit, 2 = 16 bit)\n",
    "#Méthode pour un random path\n",
    "def get_random_path():\n",
    "    dirname = './Data/Known_datas/' + str(random.choice(os.listdir('./Data/Known_datas'))) + str('/')\n",
    "    random_path = dirname + str(random.choice(os.listdir(dirname)))\n",
    "    return random_path\n",
    "\n",
    "def get_infos(path):\n",
    "    with wave.open(path) as wavefile:\n",
    "        channels_label = [\"Mono\",\"Stéréo\"]\n",
    "        framerate = wavefile.getframerate()\n",
    "        channel = wavefile.getnchannels()\n",
    "        sample = wavefile.getsampwidth()\n",
    "        return {\"Name\":path,\"Fréquence\" : str(framerate), \"Type\": str(channels_label[channel-1]), \"BitRate\" : str(sample*8)}\n",
    "\n",
    "audioInfos = get_infos(get_random_path())\n",
    "print(\"Fréquence d'échantillonnage = \"+audioInfos[\"Fréquence\"])\n",
    "print(\"Type de son = \"+audioInfos[\"Type\"])\n",
    "print(\"BitRate = \"+audioInfos[\"BitRate\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Csv des caractéristiques pour tous les enregistrements\n",
    "def create_csv_infos():\n",
    "    data_list = []\n",
    "    for dir in os.listdir('./Data/Known_datas/'):\n",
    "        for file in os.listdir('./Data/Known_datas/'+str(dir)):\n",
    "            data_list.append(get_infos('./Data/Known_datas/'+str(dir)+\"/\"+str(file)))\n",
    "    info_dataframe = pd.DataFrame(data_list)\n",
    "    info_dataframe.to_csv(\"./Data/Datas_Infos/known_Datas.csv\")\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Fonction utile pour display les fichiers audios sous forme graphique.\n",
    "heart_sound_code_to_name = {\n",
    "  'Artifact': 'Artifact, noise',\n",
    "  'Extrahs': 'Extra heart sound',\n",
    "  'Murmur': 'Murmur',  \n",
    "  'Normal': 'Normal heartbeat',\n",
    "}\n",
    "\n",
    "def show_heart_data(audio_path):\n",
    "  sample_rate, audio_data = wavfile.read(audio_path, 'rb')\n",
    "  heart_sound_code = audio_path.split('/')[-2]\n",
    "  print(f'heart sound name: {heart_sound_code_to_name[heart_sound_code]}')\n",
    "  print(f'heart sound code: {heart_sound_code}')\n",
    "\n",
    "  plttitle = f'{heart_sound_code_to_name[heart_sound_code]} ({heart_sound_code})'\n",
    "  time = np.linspace(0., audio_data.shape[0] / sample_rate,audio_data.shape[0])\n",
    "  plt.title(plttitle)\n",
    "  plt.plot(time,audio_data)\n",
    "  plt.legend()\n",
    "  plt.xlabel(\"Time [s]\")\n",
    "  plt.ylabel(\"Amplitude\")\n",
    "  plt.show()\n",
    "\n",
    "def play_audio_file(path):\n",
    "  song = AudioSegment.from_wav(path)\n",
    "  return song"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_path = get_random_path()\n",
    "\n",
    "show_heart_data(random_path)\n",
    "play(play_audio_file(random_path))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Le modèle\n",
    "- Utilisation d'un modèle pré-entrainé pour la reconnaissance audio (méthode de transfert-learning)\n",
    "- Entrainement de ce modèle sur nos données\n",
    "- Evaluation du modèle (méthode du Kappa de Cohen)\n",
    "- Test du modèle\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import modèle\n",
    "import tensorflow as tf\n",
    "import tflite_model_maker as mm\n",
    "from tflite_model_maker import audio_classifier\n",
    "import seaborn as sns\n",
    "\n",
    "import itertools\n",
    "import glob\n",
    "import random\n",
    "\n",
    "from IPython.display import Audio, Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Checkpoints are stored in C:\\Users\\basti\\AppData\\Local\\Temp\\tmp5g02r4m7\n"
     ]
    }
   ],
   "source": [
    "# Utilisation du modèle pré-entrainé\n",
    "spec = audio_classifier.YamNetSpec(keep_yamnet_and_custom_heads=True, frame_step=3 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH,frame_length=6 * audio_classifier.YamNetSpec.EXPECTED_WAVEFORM_LENGTH)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement des train_set, validation_set, test_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = '.\\\\Data\\\\Train\\\\'\n",
    "\n",
    "train_data = audio_classifier.DataLoader.from_folder(\n",
    "    spec, os.path.join(data_dir, 'train'), cache=True)\n",
    "\n",
    "validation_data = audio_classifier.DataLoader.from_folder(\n",
    "    spec, os.path.join(data_dir, 'val'), cache=True)\n",
    "\n",
    "\n",
    "test_data = audio_classifier.DataLoader.from_folder(\n",
    "    spec, os.path.join(data_dir, 'test'), cache=True)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "e67c57c2164f0d1a4b23ea8a1a6f35ed29afd7c54e555be344821e3f71753684"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
